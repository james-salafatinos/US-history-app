{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "# Setup\n",
    "baseURL = \"https://en.wikipedia.org/w/api.php?\"\n",
    "num_requests = 0\n",
    "db = {}\n",
    "args = {'depth': 1, 'pllimit': 500, 'start_title': '1931_in_the_United_States', 'write_to': 'data.gexf'}\n",
    "params = {\n",
    "    'action': \"query\",\n",
    "    'format': \"json\",\n",
    "    'prop': 'links',\n",
    "    'pllimit': args['pllimit'],\n",
    "    'plnamespace': 0,\n",
    "    'ascii': 2,\n",
    "    'titles': args['start_title'],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildURL(params, baseURL, continue_token):\n",
    "    \"\"\"\n",
    "    Takes API params and baseURL and concatenates to query string\n",
    "    \"\"\"\n",
    "    url = baseURL\n",
    "\n",
    "    if continue_token:\n",
    "        params['plcontinue'] = continue_token\n",
    "\n",
    "    for k in params:\n",
    "        url += \"&\"+k+\"=\"+str(params[k])\n",
    "    print(f'‚öôÔ∏è  - Building URL... ', url)\n",
    "    return url\n",
    "\n",
    "\n",
    "def makeRequest(url):\n",
    "    global num_requests\n",
    "    \"\"\"\n",
    "    Makes simple JSON request to API\n",
    "    https://www.mediawiki.org/wiki/API:Backlinks\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f'üì° - Requesting... (Request: ({params[\"titles\"]}), Total Requests So Far: {num_requests})')\n",
    "    num_requests += 1\n",
    "\n",
    "    # Request\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        res = r.json()\n",
    "    except:\n",
    "        print('probably server timeout...')\n",
    "        res = json.dumps({})\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def checkStop(json):\n",
    "    \"\"\"\n",
    "    Checks if continue string is returned from wikipedia\n",
    "    \"\"\"\n",
    "    stop = False\n",
    "\n",
    "    try:\n",
    "        if json['batchcomplete'] == '':\n",
    "            print('üü¢ - Batch Complete.')\n",
    "            stop = True\n",
    "    except:\n",
    "        print(\"üü° - Batch incomplete...\")\n",
    "    return stop\n",
    "\n",
    "\n",
    "def getContinueToken(json):\n",
    "    \"\"\"\n",
    "    Gets the continue token if it exists\n",
    "    \"\"\"\n",
    "    token = ''\n",
    "    try:\n",
    "        token = json['continue']['plcontinue']\n",
    "        print(\"‚û∞ - Observing continue token...\", token)\n",
    "    except:\n",
    "        \"‚óºÔ∏è - Batch complete - No continue token\"\n",
    "    return token\n",
    "\n",
    "\n",
    "def getTitles(json):\n",
    "    \"\"\"\n",
    "    Pulls the link titles out into a list\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    self_pageID = list(json['query']['pages'].keys())[0]\n",
    "\n",
    "    self_title = json['query']['pages'][self_pageID]['title']\n",
    "\n",
    "    try:\n",
    "        links = json['query']['pages'][self_pageID]['links']\n",
    "    except:\n",
    "        print(\"üî¥ - Could not get links from getTitles(json)...\")\n",
    "        links = []\n",
    "    for title in links:\n",
    "        if ok(title):\n",
    "            titles.append(title['title'])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return self_title, titles\n",
    "\n",
    "\n",
    "def ok(title):\n",
    "    \"\"\"\n",
    "    Checks for stopwords to discard links\n",
    "    \"\"\"\n",
    "    #write a regex for sentences similar to '1950 in ireland' to filter it out\n",
    "\n",
    "    #A related page, e.g., 1950 in Irelend\n",
    "    re_response = re.search('\\w{4} in', str(title))\n",
    "    if re_response:\n",
    "        return False\n",
    "\n",
    "    #Just a year e.g. 2020\n",
    "    re_response = re.search('\\A\\d{4}\\Z', str(title))\n",
    "    if re_response:\n",
    "        return False\n",
    "    \n",
    "    # returns true or false\n",
    "    return \"in the United States\" not in str(title) and \"List of\" not in str(title) and \"Wikipedia\" not in str(title) and \"Help\" not in str(title) and \"Template\" not in str(title) and \"Portal\" not in str(title) and \"Category\" not in str(title) and \"File\" not in str(title) and \"Template\" not in str(title) and \"Talk\" not in str(title) and \"Special\" not in str(title) and \"User\" not in str(title) and \"MediaWiki\" not in str(title) and \"Module\" not in str(title) and \"Book\" not in str(title) and \"Draft\" not in str(title) and \"TimedText\" not in str(title) and \"Module\" not in str(title) and \"Media\" not in str(title) and \"List of\" not in str(title) and \"Template talk\" not in str(title) and \"Category talk\" not in str(title) and \"File talk\" not in str(title) and \"Portal talk\" not in str(title) and \"Wikipedia talk\" not in str(title) and \"User talk\" not in str(title) and \"MediaWiki talk\" not in str(title) and \"Book talk\" not in str(title) and \"Draft talk\" not in str(title) and \"TimedText talk\" not in str(title) and \"Module talk\" not in str(title) and \"Media talk\" not in str(title) and \"List of\" not in str(title) and \"Template talk\" not in str(title) and \"Category talk\" not in str(title) and \"File talk\" not in str(title) and \"Portal talk\" not in str(title) and \"Wikipedia talk\" not in str(title) and \"User talk\" not in str(title) and \"MediaWiki talk\" not in str(title) and \"Book talk\" not in str(title) and \"Draft talk\" not in str(title) and \"TimedText talk\" not in str(title) and \"Module talk\" not in str(title) and \"Media talk\" not in str(title) and \"List of\" not in str(title) and \"Template talk\" not in str(title) and \"Category talk\" not in str(title) and \"File talk\" not in str(title) and \"Portal talk\" not in str(title) and \"Wikipedia talk\" not in str(title) and \"User talk\" not in str(title)\n",
    "\n",
    "\n",
    "def store(db, self_title, titles):\n",
    "    \"\"\"\n",
    "    Store in a dictionary database\n",
    "    \"\"\"\n",
    "    db[self_title] = titles\n",
    "    return None\n",
    "\n",
    "\n",
    "def merge(db, self_title, titles):\n",
    "    \"\"\"\n",
    "    Merges titles in a dictionary database\n",
    "    \"\"\"\n",
    "    db[self_title] = db[self_title] + titles\n",
    "    return None\n",
    "\n",
    "\n",
    "def dropDuplicates(db, self_title):\n",
    "    \"\"\"\n",
    "    Drops duplicate titles in a dictionary database\n",
    "    \"\"\"\n",
    "    db[self_title] = list(set(db[self_title]))\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract(self_title, continue_token=0):\n",
    "    \"\"\"\n",
    "    Recursively iterates through page continues to extract all links on a page\n",
    "    \"\"\"\n",
    "\n",
    "    res = makeRequest(buildURL(params, baseURL, continue_token))\n",
    "    continue_token = getContinueToken(res)\n",
    "    titles = getTitles(res)  # self_title, titles\n",
    "    self_title = titles[0]\n",
    "\n",
    "    # print(\"DB KEYS\", db.keys())\n",
    "\n",
    "    if self_title in db.keys():\n",
    "        print(f\"üìë - Merging... ({self_title}), ({len(titles[1])} titles)\")\n",
    "        merge(db, *titles)\n",
    "        # print(\"MERGED\", db)\n",
    "    else:\n",
    "        print(f\"üíæ - Storing... ({self_title}), ({len(titles[1])} titles)\")\n",
    "        store(db, *titles)\n",
    "        # print(\"STORED\", db)\n",
    "\n",
    "    dropDuplicates(db, self_title)\n",
    "\n",
    "    if checkStop(res):\n",
    "        return None\n",
    "\n",
    "    print('-------------------------------')\n",
    "    return extract(self_title, continue_token)\n",
    "\n",
    "\n",
    "def timeout():\n",
    "    time.sleep((np.random.randint(0,10)/10)*3.141592635897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870992)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_gexf(output_location):\n",
    "    print('‚úèÔ∏è  - Writing to .GEXF format...')\n",
    "    global db\n",
    "    import networkx as nx\n",
    "\n",
    "    in_memory_tuples = []\n",
    "    for entry in db:\n",
    "        for value in db[entry]:\n",
    "            in_memory_tuples.append((entry, value))\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(in_memory_tuples)\n",
    "    nx.write_gexf(G, output_location, encoding='utf-8', version='1.1draft')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  - Building URL...  https://en.wikipedia.org/w/api.php?&action=query&format=json&prop=links&pllimit=500&plnamespace=0&ascii=2&titles=1931_in_the_United_States\n",
      "üì° - Requesting... (Request: (1931_in_the_United_States), Total Requests So Far: 0)\n",
      "‚û∞ - Observing continue token... 24806856|0|Democratic_Party_(United_States)\n",
      "üíæ - Storing... (1931 in the United States), (214 titles)\n",
      "üü° - Batch incomplete...\n",
      "-------------------------------\n",
      "‚öôÔ∏è  - Building URL...  https://en.wikipedia.org/w/api.php?&action=query&format=json&prop=links&pllimit=500&plnamespace=0&ascii=2&titles=1931_in_the_United_States&plcontinue=24806856|0|Democratic_Party_(United_States)\n",
      "üì° - Requesting... (Request: (1931_in_the_United_States), Total Requests So Far: 1)\n",
      "‚û∞ - Observing continue token... 24806856|0|New_York_(state)\n",
      "üìë - Merging... (1931 in the United States), (496 titles)\n",
      "üü° - Batch incomplete...\n",
      "-------------------------------\n",
      "‚öôÔ∏è  - Building URL...  https://en.wikipedia.org/w/api.php?&action=query&format=json&prop=links&pllimit=500&plnamespace=0&ascii=2&titles=1931_in_the_United_States&plcontinue=24806856|0|New_York_(state)\n",
      "üì° - Requesting... (Request: (1931_in_the_United_States), Total Requests So Far: 2)\n",
      "üìë - Merging... (1931 in the United States), (214 titles)\n",
      "üü¢ - Batch Complete.\n",
      "‚úèÔ∏è  - Writing to .GEXF format...\n"
     ]
    }
   ],
   "source": [
    "extract(args['start_title'])\n",
    "\n",
    "if args['write_to']:\n",
    "    if not os.path.exists(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "    write_to_gexf(\"./data/\" + args['write_to'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx   \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def loadGEXF(gexf_file):\n",
    "    G = nx.read_gexf(gexf_file)\n",
    "    return G\n",
    "\n",
    "G = loadGEXF(\"./data/\" + args['write_to'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "869037e4c7181581fc9fcd094941b6eefc4ba9e2c92f502817da3673f02682d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
